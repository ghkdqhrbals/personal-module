package org.ghkdqhrbals.client.domain.event

import com.fasterxml.jackson.databind.ObjectMapper
import com.fasterxml.jackson.module.kotlin.readValue
import jakarta.annotation.PostConstruct
import jakarta.annotation.PreDestroy
import kotlinx.coroutines.*
import org.ghkdqhrbals.client.ai.LlmClient
import org.ghkdqhrbals.client.config.Jackson
import org.ghkdqhrbals.client.config.log.logger
import org.ghkdqhrbals.client.config.log.LogTitle
import org.ghkdqhrbals.client.config.log.title
import org.ghkdqhrbals.client.domain.paper.repository.PaperRepository
import org.springframework.beans.factory.annotation.Value
import org.springframework.data.redis.connection.stream.Consumer
import org.springframework.data.redis.connection.stream.MapRecord
import org.springframework.data.redis.connection.stream.ReadOffset
import org.springframework.data.redis.connection.stream.StreamOffset
import org.springframework.data.redis.core.StringRedisTemplate
import org.springframework.data.redis.stream.StreamListener
import org.springframework.data.redis.stream.StreamMessageListenerContainer
import org.springframework.stereotype.Component
import java.time.Duration
import java.time.OffsetDateTime
import kotlinx.serialization.json.Json


/**
 * SummaryEvent ë¦¬ìŠ¤ë„ˆ
 * - LLMìœ¼ë¡œ ë…¼ë¬¸ ìš”ì•½ ìƒì„±
 * - DB ì—…ë°ì´íŠ¸
 * - Redis ì§„í–‰ìƒíƒœ ì—…ë°ì´íŠ¸
 */
@Component
class SummaryStreamListener(
    private val redisTemplate: StringRedisTemplate,
    private val llmClient: LlmClient,
    private val paperRepository: PaperRepository,
    @Value("\${redis.stream.events.summary:domain:events:summary}") private val streamKey: String,
    @Value("\${redis.stream.events.group:event-handlers}") private val groupName: String,
    @Value("\${redis.stream.summary.consumer-count:20}") private val consumerCount: Int,
    @Value("\${redis.stream.summary.poll-timeout-ms:3000}") private val pollTimeoutMs: Long,
    @Value("\${redis.stream.summary.consumer-name-prefix:summary-consumer}") private val consumerNamePrefix: String,
) : StreamListener<String, MapRecord<String, String, String>> {

    private val mapper: ObjectMapper = Jackson.getMapper()
    private lateinit var container: StreamMessageListenerContainer<String, MapRecord<String, String, String>>

    @PostConstruct
    fun start() {
        initializeConsumerGroup()

        container = StreamMessageListenerContainer.create(
            redisTemplate.connectionFactory!!,
            StreamMessageListenerContainer.StreamMessageListenerContainerOptions.builder()
                .pollTimeout(Duration.ofMillis(pollTimeoutMs))
                .build()
        )

        // consumerCount ê°œìˆ˜ë§Œí¼ ì—¬ëŸ¬ ì»¨ìŠˆë¨¸ ë“±ë¡
        repeat(consumerCount) { index ->
            val consumerName = "$consumerNamePrefix-$index"
            container.receive(
                Consumer.from(groupName, consumerName),
                StreamOffset.create(streamKey, ReadOffset.lastConsumed()),
                this
            )
            logger().title(LogTitle.STREAM, "Registered consumer name=$consumerName for stream=$streamKey")
        }

        container.start()
        logger().title(LogTitle.STREAM, "Started listening on stream=$streamKey, consumerCount=$consumerCount, pollTimeoutMs=$pollTimeoutMs")
    }

    private fun initializeConsumerGroup() {
        runCatching {
            if (!redisTemplate.hasKey(streamKey)) {
                redisTemplate.opsForStream<String, String>()
                    .add(streamKey, mapOf("init" to "bootstrap"))
                logger().title(LogTitle.STREAM, "Created stream: $streamKey")
            }
            redisTemplate.opsForStream<String, String>()
                .createGroup(streamKey, ReadOffset.latest(), groupName)
            logger().title(LogTitle.STREAM, "Created consumer group: $groupName")
        }.onFailure {
            logger().debug("[SummaryListener] Consumer group already exists: $groupName")
        }
    }

    override fun onMessage(message: MapRecord<String, String, String>) {
        // receive ~ ì²˜ë¦¬ ~ ACK ë¥¼ í•˜ë‚˜ì˜ ë™ê¸° íë¦„ìœ¼ë¡œ ì²˜ë¦¬
        runBlocking {
            handleSummaryEvent(message)
        }
    }

    private suspend fun handleSummaryEvent(message: MapRecord<String, String, String>) {
        logger().title(LogTitle.SUMMARY, "Received message: id=${message.id}")

        var searchEventId: String? = null
        var paperId: String? = null

        try {
            val payload = message.value["payload"] ?: run {
                logger().warn("[SummaryListener] Empty payload in message: id=${message.id}")
                acknowledge(message)
                return
            }

            val event: SummaryEvent = try {
                mapper.readValue(payload)
            } catch (e: Exception) {
                logger().error("[SummaryListener] âŒ Failed to parse event payload", e)
                logger().error("[SummaryListener] Payload (first 500 chars): ${payload.take(500)}")
                acknowledge(message)
                return
            }

            searchEventId = event.searchEventId
            paperId = event.paperId

            logger().title(LogTitle.SUMMARY,"Processing summary for paperId=${paperId}, searchEventId=${searchEventId}")

            val startTime = System.currentTimeMillis()


            // LLM í˜¸ì¶œ (IO ì‘ì—…)
            val analysis = try {
                llmClient.summarizePaper(
                    event.abstract ?: "",
                    event.maxLength,
                    event.journalRefRaw
                )
            } catch (e: IllegalStateException) {
                logger().error("[SummaryListener] âŒ LLM processing failed for paperId=${paperId}: ${e.message}", e)
                incrementProgress(searchEventId, "failed")
                checkAndMarkCompleted(searchEventId)
                acknowledge(message)
                return
            } catch (e: com.fasterxml.jackson.core.JsonProcessingException) {
                logger().error("[SummaryListener] âŒ JSON parsing failed for LLM response, paperId=${paperId}", e)
                incrementProgress(searchEventId, "failed")
                checkAndMarkCompleted(searchEventId)
                acknowledge(message)
                return
            } catch (e: Exception) {
                logger().error("[SummaryListener] âŒ Unexpected error during LLM call for paperId=${paperId}", e)
                incrementProgress(searchEventId, "failed")
                checkAndMarkCompleted(searchEventId)
                acknowledge(message)
                return
            }

            val duration = System.currentTimeMillis() - startTime
            logger().title(LogTitle.SUMMARY, "LLM completed in ${duration}ms for paperId=${paperId}")

            // DB ì—…ë°ì´íŠ¸ - arxivIdë¡œ Paper ì°¾ê¸°
            val arxivId = event.arxivId
            if (arxivId.isNullOrBlank()) {
                logger().warn("[SummaryListener] âš ï¸ No arxivId in event for paperId=${paperId}, skipping DB update")
                incrementProgress(searchEventId, "failed")
                checkAndMarkCompleted(searchEventId)
                acknowledge(message)
                return
            }

            val paper = paperRepository.findByArxivId(arxivId)

            if (paper == null) {
                logger().warn("[SummaryListener] âš ï¸ Paper not found for arxivId=$arxivId, skipping (may not be saved yet)")
                incrementProgress(searchEventId, "failed")
                checkAndMarkCompleted(searchEventId)
                acknowledge(message)
                return
            }

            // Paperì— ìš”ì•½ ì •ë³´ ì—…ë°ì´íŠ¸
            val updated = paper.copy(
                summary = analysis.coreContribution,
                novelty = analysis.noveltyAgainstPreviousWorks,
                summarizedAt = OffsetDateTime.now(),
                journal = analysis.journalName ?: paper.journal,
                impactFactor = analysis.impactFactor ?: paper.impactFactor
            )

            paperRepository.save(updated)


            logger().title(LogTitle.PAPER, "âœ… Updated paper summary: arxivId=$arxivId")
            logger().debug("   â”œâ”€ Core: ${analysis.coreContribution.take(50)}...")
            logger().debug("   â”œâ”€ Novelty: ${analysis.noveltyAgainstPreviousWorks.take(50)}...")
            if (analysis.journalName != null) {
                logger().debug("   â””â”€ Journal: ${analysis.journalName} (IF: ${analysis.impactFactor})")
            }

            // ì§„í–‰ìƒíƒœ ì—…ë°ì´íŠ¸ (ì„±ê³µ)
            incrementProgress(searchEventId, "completed")

            // ì™„ë£Œ ì²´í¬
            checkAndMarkCompleted(searchEventId)

            // ACK
            acknowledge(message)

            logger().title(LogTitle.SUMMARY, "âœ… Completed summary for paperId=${paperId}")

        } catch (e: Exception) {
            logger().error("[SummaryListener] âŒ Unexpected error processing message: id=${message.id}", e)

            searchEventId?.let {
                try {
                    incrementProgress(it, "failed")
                    checkAndMarkCompleted(it)
                } catch (progressError: Exception) {
                    logger().error("[SummaryListener] Failed to update progress", progressError)
                }
            }

            acknowledge(message)
        }
    }

    private fun acknowledge(message: MapRecord<String, String, String>) {
        try {
            redisTemplate.opsForStream<String, String>()
                .acknowledge(streamKey, groupName, message.id)
        } catch (e: Exception) {
            logger().error("[SummaryListener] Failed to acknowledge message: id=${message.id}", e)
        }
    }

    private fun incrementProgress(searchEventId: String, field: String) {
        val key = "search:$searchEventId:progress"
        redisTemplate.opsForHash<String, String>().increment(key, field, 1)
        redisTemplate.expire(key, 3600, java.util.concurrent.TimeUnit.SECONDS)
        logger().debug("[SummaryListener] Incremented $field for searchEventId=$searchEventId")
    }

    private fun checkAndMarkCompleted(searchEventId: String) {
        val key = "search:$searchEventId:progress"
        val entries = redisTemplate.opsForHash<String, String>().entries(key)

        val total = entries["total"]?.toIntOrNull() ?: 0
        val completed = entries["completed"]?.toIntOrNull() ?: 0
        val failed = entries["failed"]?.toIntOrNull() ?: 0

        // ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸
        if (total > 0 && (completed + failed) >= total) {
            val status = entries["status"]
            // ì•„ì§ ì™„ë£Œë¡œ ë§ˆí¬ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì™„ë£Œ ì²˜ë¦¬
            if (status != "COMPLETED") {
                redisTemplate.opsForHash<String, String>().put(key, "status", "COMPLETED")
                redisTemplate.expire(key, 3600, java.util.concurrent.TimeUnit.SECONDS)
                logger().title(LogTitle.SUMMARY, "ğŸ‰ All summaries completed for searchEventId=$searchEventId (total=$total, completed=$completed, failed=$failed)")
            }
        }
    }

    @PreDestroy
    fun shutdown() {
        container.stop()
        logger().title(LogTitle.STREAM, "Stopped")
    }
}
