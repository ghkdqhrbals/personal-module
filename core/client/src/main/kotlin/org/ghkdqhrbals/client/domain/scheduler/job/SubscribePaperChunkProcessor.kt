package org.ghkdqhrbals.client.domain.scheduler.job

import kotlinx.coroutines.runBlocking
import org.ghkdqhrbals.client.ai.LlmClient
import org.ghkdqhrbals.client.common.LockTimeoutException
import org.ghkdqhrbals.repository.paper.PaperRepository
import org.ghkdqhrbals.client.domain.paper.service.ArxivApiException
import org.ghkdqhrbals.client.domain.paper.service.ArxivHttpClient
import org.ghkdqhrbals.client.domain.paper.service.ArxivService
import org.ghkdqhrbals.repository.subscribe.Subscribe
import org.ghkdqhrbals.model.paper.PaperSearchAndStoreEvent
import org.ghkdqhrbals.repository.subscribe.SubscribeType
import org.slf4j.LoggerFactory
import org.springframework.data.domain.PageRequest
import org.springframework.stereotype.Component
import java.time.OffsetDateTime
import java.util.UUID

/**
 * ê° Subscribeì— ëŒ€í•´ íŽ˜ì´ì§€ë„¤ì´ì…˜ ê¸°ë°˜ìœ¼ë¡œ ë…¼ë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” ì²­í¬ í”„ë¡œì„¸ì„œ
 */
@Component
class SubscribePaperChunkProcessor(
    private val arxivService: ArxivService,
    private val httpClient: ArxivHttpClient,
    private val llmClient: LlmClient,
    private val paperRepository: PaperRepository,
) {
    private val logger = LoggerFactory.getLogger(SubscribePaperChunkProcessor::class.java)

    /**
     * Subscribe í•˜ë‚˜ì— ëŒ€í•´ ëª¨ë“  íŽ˜ì´ì§€ì˜ ë…¼ë¬¸ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
     *
     * @param subscribe êµ¬ë… ì •ë³´
     * @param pageSize íŽ˜ì´ì§€ë‹¹ ë…¼ë¬¸ ìˆ˜
     * @param maxConsecutiveFailures ì—°ì† ì‹¤íŒ¨ í—ˆìš© íšŸìˆ˜
     * @return ì´ ì²˜ë¦¬ëœ ë…¼ë¬¸ ìˆ˜
     */
    fun processAllPages(
        subscribe: Subscribe,
        pageSize: Int = 10,
        maxConsecutiveFailures: Int = 3
    ): Int {
        val subscribeInfo = "[Subscribe#${subscribe.id}] '${subscribe.name}' (${subscribe.subscribeType})"

        logger.info("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        logger.info("â•‘ ðŸ“š êµ¬ë… ì²˜ë¦¬ ì‹œìž‘")
        logger.info("â•‘ â”œâ”€ êµ¬ë… ID: ${subscribe.id}")
        logger.info("â•‘ â”œâ”€ êµ¬ë… ì´ë¦„: ${subscribe.name}")
        logger.info("â•‘ â”œâ”€ êµ¬ë… íƒ€ìž…: ${subscribe.subscribeType}")
        logger.info("â•‘ â””â”€ íŽ˜ì´ì§€ í¬ê¸°: ${pageSize}ê°œ/íŽ˜ì´ì§€")
        logger.info("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

        var currentPage = 0
        var totalProcessed = 0
        var consecutiveFailures = 0
        var totalSkippedPages = 0

        try {
            while (true) {
                val pageInfo = "Page#${currentPage}"

                try {
                    logger.info("ðŸ” $subscribeInfo $pageInfo - ArXiv ê²€ìƒ‰ ì‹œìž‘...")

                    // í˜„ìž¬ íŽ˜ì´ì§€ì˜ ë…¼ë¬¸ ê²€ìƒ‰
                    val processedCount = processPage(subscribe, currentPage, pageSize)

                    // ë” ì´ìƒ ë…¼ë¬¸ì´ ì—†ìœ¼ë©´ ì¤‘ë‹¨
                    if (processedCount == 0) {
                        logger.info("âœ“ $subscribeInfo $pageInfo - ë…¼ë¬¸ ì—†ìŒ (ê²€ìƒ‰ ì¢…ë£Œ)")
                        break
                    }

                    totalProcessed += processedCount
                    consecutiveFailures = 0 // ì„±ê³µ ì‹œ ì¹´ìš´í„° ë¦¬ì…‹
                    currentPage++

                    logger.info("âœ… $subscribeInfo $pageInfo - ì„±ê³µ: ${processedCount}ê°œ ë…¼ë¬¸ ì²˜ë¦¬ | ëˆ„ì : ${totalProcessed}ê°œ")

                } catch (e: LockTimeoutException) {
                    consecutiveFailures++
                    val failureInfo = "ì—°ì† ì‹¤íŒ¨: ${consecutiveFailures}/${maxConsecutiveFailures}"

                    logger.warn("âš ï¸  $subscribeInfo $pageInfo - Rate Limit íƒ€ìž„ì•„ì›ƒ ($failureInfo)")
                    logger.warn("   â””â”€ ì‚¬ìœ : ${e.message}")

                    if (consecutiveFailures >= maxConsecutiveFailures) {
                        logger.error("âŒ $subscribeInfo - ì—°ì† ${consecutiveFailures}íšŒ ì‹¤íŒ¨ë¡œ êµ¬ë… ì²˜ë¦¬ ì¤‘ë‹¨")
                        break
                    }

                    // Rate limitì´ë¯€ë¡œ ìž ì‹œ ëŒ€ê¸° í›„ ìž¬ì‹œë„
                    val waitTime = 5000L * consecutiveFailures
                    logger.info("â³ $subscribeInfo $pageInfo - ${waitTime}ms ëŒ€ê¸° í›„ ìž¬ì‹œë„...")
                    Thread.sleep(waitTime)
                    // ê°™ì€ íŽ˜ì´ì§€ ìž¬ì‹œë„

                } catch (e: ArxivApiException) {
                    consecutiveFailures++
                    totalSkippedPages++
                    val failureInfo = "ì—°ì† ì‹¤íŒ¨: ${consecutiveFailures}/${maxConsecutiveFailures}"

                    logger.error("âš ï¸  $subscribeInfo $pageInfo - ArXiv API ì—ëŸ¬ ($failureInfo)")
                    logger.error("   â””â”€ ì‚¬ìœ : ${e.message}")

                    if (consecutiveFailures >= maxConsecutiveFailures) {
                        logger.error("âŒ $subscribeInfo - ì—°ì† ${consecutiveFailures}íšŒ ì‹¤íŒ¨ë¡œ êµ¬ë… ì²˜ë¦¬ ì¤‘ë‹¨")
                        break
                    }

                    logger.info("â­ï¸  $subscribeInfo $pageInfo - íŽ˜ì´ì§€ ìŠ¤í‚µ í›„ ë‹¤ìŒ íŽ˜ì´ì§€ë¡œ ì´ë™")
                    currentPage++

                } catch (e: Exception) {
                    consecutiveFailures++
                    totalSkippedPages++
                    val failureInfo = "ì—°ì† ì‹¤íŒ¨: ${consecutiveFailures}/${maxConsecutiveFailures}"

                    logger.error("âš ï¸  $subscribeInfo $pageInfo - ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ($failureInfo)", e)

                    if (consecutiveFailures >= maxConsecutiveFailures) {
                        logger.error("âŒ $subscribeInfo - ì—°ì† ${consecutiveFailures}íšŒ ì‹¤íŒ¨ë¡œ êµ¬ë… ì²˜ë¦¬ ì¤‘ë‹¨")
                        break
                    }

                    logger.info("â­ï¸  $subscribeInfo $pageInfo - íŽ˜ì´ì§€ ìŠ¤í‚µ í›„ ë‹¤ìŒ íŽ˜ì´ì§€ë¡œ ì´ë™")
                    currentPage++
                }
            }

            logger.info("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
            logger.info("â•‘ ðŸŽ¯ êµ¬ë… ì²˜ë¦¬ ì™„ë£Œ")
            logger.info("â•‘ â”œâ”€ êµ¬ë…: ${subscribe.name} (${subscribe.subscribeType})")
            logger.info("â•‘ â”œâ”€ ì²˜ë¦¬ëœ ë…¼ë¬¸: ${totalProcessed}ê°œ")
            logger.info("â•‘ â”œâ”€ ì²˜ë¦¬ëœ íŽ˜ì´ì§€: ${currentPage}ê°œ")
            logger.info("â•‘ â”œâ”€ ìŠ¤í‚µëœ íŽ˜ì´ì§€: ${totalSkippedPages}ê°œ")
            logger.info("â•‘ â””â”€ ìµœì¢… ì‹¤íŒ¨ íšŸìˆ˜: ${consecutiveFailures}íšŒ")
            logger.info("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

        } catch (e: Exception) {
            logger.error("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
            logger.error("â•‘ âŒ êµ¬ë… ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜")
            logger.error("â•‘ â”œâ”€ êµ¬ë…: ${subscribe.name}")
            logger.error("â•‘ â””â”€ ì˜¤ë¥˜: ${e.message}")
            logger.error("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•", e)
            throw e
        }

        return totalProcessed
    }

    /**
     * íŠ¹ì • íŽ˜ì´ì§€ì˜ ë…¼ë¬¸ì„ ì²˜ë¦¬
     *
     * @return ì²˜ë¦¬ëœ ë…¼ë¬¸ ìˆ˜
     */
    private fun processPage(subscribe: Subscribe, page: Int, pageSize: Int): Int {
        val pageRequest = PageRequest.of(page, pageSize)
        val event = subscribe.toPaperSearchAndStoreEvent(pageRequest)

        // ArXivì—ì„œ ë…¼ë¬¸ ê²€ìƒ‰ ë° ì‹ ê·œë…¼ë¬¸ ì €ìž¥
        val papers = arxivService.analyze(event)?: return 0
        val map = papers.keys.map { it.toSummaryEvent() }

        logger.debug("   â””â”€ ê²€ìƒ‰ ê²°ê³¼: ${papers.size}ê°œ ë…¼ë¬¸ ë°œê²¬")

        // ì§ì ‘ ìš”ì•½ ì‹¤í–‰
        runBlocking {
            map.forEach { event ->
                val analysis = llmClient.summarizePaper(
                    event.abstract ?: "",
                    150,
                    event.journalRefRaw
                )

                // ì´ê±¸ë¡œ paperRepository ì—…ë°ì´íŠ¸.
                val paper = paperRepository.findByArxivId(event.arxivId!!)
                val updated = paper!!.copy(
                    summary = analysis.coreContribution,
                    novelty = analysis.noveltyAgainstPreviousWorks,
                    summarizedAt = OffsetDateTime.now(),
                    journal = analysis.journalName ?: paper.journal,
                    impactFactor = analysis.impactFactor ?: paper.impactFactor
                )
                paperRepository.save(updated)
            }

        }

        return papers.size
    }
}

/**
 * Subscribeë¥¼ PaperSearchAndStoreEventë¡œ ë³€í™˜í•˜ëŠ” í™•ìž¥ í•¨ìˆ˜
 */
private fun Subscribe.toPaperSearchAndStoreEvent(page: PageRequest): PaperSearchAndStoreEvent {
    // Subscribe íƒ€ìž…ì— ë”°ë¼ ë‹¤ë¥¸ ì¿¼ë¦¬ ìƒì„±
    val query = when (this.subscribeType) {
        SubscribeType.CATEGORY -> {
            "cat:${this.name}"
        }
        SubscribeType.KEYWORD -> {
            "all:${this.name}"
        }
        SubscribeType.AUTHOR -> {
            "au:${this.name}"
        }
        else -> {
            this.name
        }
    }

    return PaperSearchAndStoreEvent(
        searchEventId = UUID.randomUUID().toString(),
        query = query,
        categories = null,
        maxResults = page.pageSize,
        page = page.pageNumber,
        fromDate = null
    )
}

