package org.ghkdqhrbals.client.domain.paper.service

import org.ghkdqhrbals.client.config.log.logger
import org.ghkdqhrbals.client.controller.paper.dto.*
import org.ghkdqhrbals.repository.paper.PaperEntity
import org.ghkdqhrbals.repository.paper.PaperRepository
import org.ghkdqhrbals.model.paper.PaperSearchAndStoreEvent
import org.springframework.data.redis.core.StringRedisTemplate
import org.springframework.stereotype.Service
import org.springframework.transaction.annotation.Propagation
import org.springframework.transaction.annotation.Transactional
import java.util.UUID

/**
 * arXiv ë…¼ë¬¸ ê²€ìƒ‰ ì„œë¹„ìŠ¤ (ë‹¨ìˆœí™” ë²„ì „)
 * - ê²€ìƒ‰ ìš”ì²­ì„ ì´ë²¤íŠ¸ë¡œ ë°œí–‰í•˜ê³  ì¦‰ì‹œ eventId ë°˜í™˜
 * - ì‹¤ì œ ê²€ìƒ‰/ì €ì¥ì€ PaperSearchAndStoreStreamListenerê°€ ì²˜ë¦¬
 * - ì§„í–‰ìƒíƒœëŠ” Redisì—ì„œ ì§ì ‘ ì¡°íšŒ (í”„ë¡œì ì…˜ ì—†ì´ ë‹¨ì¼ raw ë°ì´í„° ì‚¬ìš©)
 */
@Service
class ArxivService(
    private val paperRepository: PaperRepository,
    private val redisTemplate: StringRedisTemplate,
    private val arxivHttpClient: ArxivHttpClient,
) {
    fun existsById(arxivId: String): Boolean {
        return paperRepository.existsByArxivId(arxivId)
    }

    /**
     * ì‹ ê·œ ë…¼ë¬¸ ë°œê²¬ ì‹œ ì €ì¥í•˜ê³  SummaryEvent ë°˜í™˜
     */
    @Transactional(propagation = Propagation.REQUIRES_NEW)
    fun analyze(event: PaperSearchAndStoreEvent): Map<ArxivPaper, PaperEntity?>? {
        val returnMaps = mutableMapOf<ArxivPaper, PaperEntity?>()

        val response = arxivHttpClient.search(event)

        // 1. ì´ë²ˆ ì‘ë‹µì—ì„œ ì˜¨ arxivIdë“¤ë§Œ ìˆ˜ì§‘
        val incomingIds = response.map { it.arxivId }
        if (incomingIds.isEmpty()) return null

        // 2. ê·¸ arxivIdë“¤ ì¤‘ì—ì„œ ì´ë¯¸ DBì— ì¡´ì¬í•˜ëŠ” ì• ë“¤ë§Œ í•œ ë²ˆì— ì¡°íšŒ
        val papers = paperRepository.findAllByArxivIdIn(incomingIds)
        val existingIds: Set<String?> = papers.map { it.arxivId }.toSet()

        // ì´ë¯¸ ìˆëŠ” ë…¼ë¬¸ ë„£ê¸°. ì—†ìœ¼ë©´ Null
        returnMaps.putAll(
            response.associateWith { paper ->
                papers.firstOrNull { it.arxivId == paper.arxivId }
            }
        )

        // returnMaps ì—ì„œ PaperEntity null ì¸ ì• ë“¤
        val newPapers = returnMaps.filterValues { it == null }

        if (newPapers.isEmpty()) {
            logger().info("ì‹ ê·œ ë…¼ë¬¸ ì—†ìŒ. totalResponse=${incomingIds.size}")
            return null
        }

        logger().info("ğŸ“„ ì‹ ê·œ ë…¼ë¬¸ ${newPapers.size}ê±´ ë°œê²¬. totalResponse=${incomingIds.size}")

        val saves = paperRepository.saveAll(newPapers.keys.map { it.toPaperEntity() })

        returnMaps.replaceAll { k, v -> v ?: saves.firstOrNull { it.arxivId == k.arxivId } }
        return returnMaps
    }

    /**
     * ë¹„ë™ê¸° arXiv ê²€ìƒ‰ ì‹œì‘ - ì´ë²¤íŠ¸ IDë§Œ ì¦‰ì‹œ ë°˜í™˜
     */
    @Transactional
    fun searchAsync(
        query: String,
        categories: List<String>? = null,
        maxResults: Int = 10,
        page: Int = 0,
        fromDate: String? = null,
        summarize: Boolean = true
    ): String {
        val searchEventId = UUID.randomUUID().toString()

        try {
            // PaperSearchAndStoreEvent ë°œí–‰
            val event = PaperSearchAndStoreEvent(
                searchEventId = searchEventId,
                query = query,
                categories = categories,
                maxResults = maxResults,
                page = page,
                fromDate = fromDate,
                shouldSummarize = summarize,
            )
            // ì—¬ê¸°ì„œ ì´ë²¤íŠ¸ ì˜ë©´ ë¨.

            logger().info(
                "[ArxivService] Published PaperSearchAndStoreEvent: " +
                "searchEventId=$searchEventId, query=$query, categories=$categories, summarize=$summarize"
            )
        } catch (e: Exception) {
            logger().error("Failed to publish PaperSearchAndStoreEvent for searchEventId=$searchEventId", e)

            // ì‹¤íŒ¨ ìƒíƒœë¥¼ Redisì— ì €ì¥
            redisTemplate.opsForHash<String, String>().putAll(
                "search:$searchEventId:progress",
                mapOf(
                    "status" to "FAILED",
                    "error" to (e.message ?: "Unknown error")
                )
            )
            redisTemplate.expire("search:$searchEventId:progress", 3600, java.util.concurrent.TimeUnit.SECONDS)
        }

        return searchEventId
    }

    /**
     * ê²€ìƒ‰ ìƒíƒœ ì¡°íšŒ
     * Redisì—ì„œ ë‹¨ì¼ raw ë°ì´í„°ë¥¼ ì§ì ‘ ì¡°íšŒ (í”„ë¡œì ì…˜ ì—†ì´ íš¨ìœ¨ì )
     */
    fun getSearchStatus(searchEventId: String): ArxivSearchStatusResponse {
        val progressKey = "search:$searchEventId:progress"
        val progressEntries = redisTemplate.opsForHash<String, String>().entries(progressKey)

        if (progressEntries.isEmpty()) {
            return ArxivSearchStatusResponse(
                eventId = searchEventId,
                status = SearchStatus.NOT_FOUND,
                batch = null,
                summary = null,
                papers = null
            )
        }

        val status = progressEntries["status"] ?: "PENDING"
        val total = progressEntries["total"]?.toIntOrNull() ?: 0
        val completed = progressEntries["completed"]?.toIntOrNull() ?: 0
        val failed = progressEntries["failed"]?.toIntOrNull() ?: 0
        val processing = (total - completed - failed).coerceAtLeast(0)
        val progressPercent = if (total > 0) {
            (completed + failed).toDouble() / total.toDouble() * 100.0
        } else {
            0.0
        }
        val isDone = status == "COMPLETED" || status == "FAILED" || (total > 0 && (completed + failed) >= total)

        val searchStatus = when (status) {
            "COMPLETED" -> SearchStatus.COMPLETED
            "FAILED" -> SearchStatus.FAILED
            "IN_PROGRESS" -> SearchStatus.IN_PROGRESS
            else -> SearchStatus.PENDING
        }

        // ë…¼ë¬¸ ëª©ë¡ ì¡°íšŒ (ìµœê·¼ 100ê°œë¡œ ì„ì‹œ)
        val papers = if (total > 0) {
            paperRepository.findTop100ByOrderBySearchDateDesc().map { entity ->
                Paper(
                    title = entity.title,
                    authors = entity.author?.split(",")?.map { it.trim() } ?: emptyList(),
                    journal = entity.journal,
                    publicationDate = entity.publishedAt?.toString(),
                    doi = null,
                    abstract = null,
                    url = entity.url,
                    citations = null,
                    impactFactor = entity.impactFactor,
                    summary = entity.summary,
                    novelty = entity.novelty
                )
            }
        } else {
            null
        }

        logger().info(
            "[ArxivService] Search status for searchEventId=$searchEventId: " +
            "status=$status, total=$total, completed=$completed, failed=$failed"
        )

        return ArxivSearchStatusResponse(
            eventId = searchEventId,
            status = searchStatus,
            batch = BatchInfo(
                totalPapers = total,
                category = null,
                startedAt = null
            ),
            summary = SummaryInfo(
                total = total,
                completed = completed,
                failed = failed,
                processing = processing,
                progressPercent = String.format("%.2f", progressPercent).toDouble(),
                isDone = isDone
            ),
            papers = papers
        )
    }


}
